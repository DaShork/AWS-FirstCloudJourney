[{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Event Report: AWS Cloud Mastery Series #1 : AI/ML/GenAI on AWS Workshop Purpose of the Event Demystify AI for Practical Application: To break down the complex worlds of Machine Learning and Generative AI into understandable, actionable concepts for developers, architects, and tech enthusiasts. Showcase the AWS AI/ML Stack: To provide a comprehensive overview of AWS\u0026rsquo;s key services, demonstrating how Amazon SageMaker covers the end-to-end traditional ML lifecycle and how Amazon Bedrock simplifies access to powerful Generative AI. Equip with Hands-On Skills: To move beyond presentations and provide practical knowledge through live demonstrations, showing attendees exactly how to navigate SageMaker Studio and build a functional GenAI chatbot with Bedrock. Connect Theory to Real-World Value: To illustrate how these advanced technologies can be leveraged to solve business problems, enhance products, and drive innovation within the Vietnamese market. List of Speakers Dinh Le Hoang Anh - AWS Vietnam Lam Tuan Kiet - AWS Vietnam Danh Hoang Hieu Nghi - AWS Vietnam Key Content \u0026amp; Highlights The Complete Machine Learning Lifecycle with Amazon SageMaker: Focus: The first half of the workshop provided a structured, end-to-end journey through a typical machine learning project. Content: The session covered every critical stage, from data preparation and labeling to model training, hyperparameter tuning, and seamless deployment. A key emphasis was placed on MLOps, showcasing how SageMaker integrates tools to automate and manage the entire lifecycle efficiently. Key Moment: The live demo of SageMaker Studio was a major highlight. It brought the concepts to life, giving attendees a clear visual walkthrough of the unified environment and demystifying the complexity of managing an ML workflow. Related Session: \u0026ldquo;AWS AI/ML Services Overview\u0026rdquo;. A Practical Deep Dive into Generative AI with Amazon Bedrock: Focus: This session pivoted to the cutting edge, exploring the transformative potential of Generative AI in a highly accessible way. Content: The presentation masterfully covered the core components of building with GenAI on AWS: Foundation Models: A clear guide on how to compare and select the right model (like Claude, Llama, Titan) for specific tasks. Prompt Engineering: Practical techniques to communicate effectively with models, including advanced methods like Chain-of-Thought reasoning. Retrieval-Augmented Generation (RAG): An architectural overview of how to connect models to private knowledge bases, making them smarter and more context-aware. Bedrock Agents \u0026amp; Guardrails: A look into building autonomous agents that can perform multi-step tasks and the crucial importance of implementing safety filters. Key Moment: The climax of the morning was the live demo of building a GenAI chatbot using Bedrock. In a matter of minutes, the speaker created a functional, intelligent application, proving how quickly developers can go from idea to prototype. Related Session: \u0026ldquo;Generative AI with Amazon Bedrock\u0026rdquo;. What You Learned On Traditional Machine Learning End-to-End Project Management: You learned how to use Amazon SageMaker as a single, unified platform to manage the entire ML workflow. You now have a clear understanding of the steps required to take a project from raw data to a deployed model in production. MLOps for Efficiency: You grasped the importance of MLOps and saw how SageMaker\u0026rsquo;s integrated capabilities can help automate, monitor, and govern machine learning models at scale, reducing manual effort and improving reliability. On Generative AI How to Choose and Use Foundation Models: You gained practical criteria for selecting the best foundation model for your needs and learned fundamental prompt engineering techniques to get high-quality, relevant responses. Making AI Smarter with Your Data: You now understand the concept and architecture of RAG, giving you a clear path to building applications that can reason over your company\u0026rsquo;s private documents and data. Building and Securing AI Applications: You learned how Bedrock Agents can automate complex workflows and how to use Guardrails to ensure your AI applications are safe, responsible, and aligned with your business policies. Experience at the Event Spending a Saturday morning at the AWS Vietnam office for this workshop felt like a powerful investment in the future. The atmosphere was focused and energetic, filled with a shared desire to understand and harness the power of AI.\nBuilding a Solid Foundation The session on SageMaker was incredibly grounding. Instead of abstract concepts, we were walked through a logical, structured process that made the entire machine learning lifecycle feel manageable. The SageMaker Studio demo was the \u0026ldquo;aha!\u0026rdquo; moment, transforming a complex diagram on a slide into a tangible, navigable tool. You left that session feeling that building and deploying a machine learning model was no longer a mysterious art, but a clear engineering discipline.\nA Thrilling Leap into GenAI The transition to the Amazon Bedrock session was palpable—the energy in the room shifted towards excitement and wonder. The speaker did a fantastic job of breaking down what could be intimidating topics (like RAG and Agents) into simple, powerful ideas. But the true magic was the live chatbot demo. Witnessing a sophisticated AI assistant come to life so quickly and easily was incredibly inspiring. It wasn\u0026rsquo;t just a demonstration of a product; it was a demonstration of possibility.\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Vo Nhat Minh\nPhone Number: (86)0906304988\nEmail: nhatminhvo2311@gmail.com\nUniversity: FPT University HCM\nMajor: Software Engineering\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Artificial Intelligence: Integrating Tokenization with Amazon Bedrock Guardrails for Secure Data Processing Written by Nizar Kheir and Mark Warner | September 23, 2025 | Category: Amazon API Gateway, Amazon Bedrock Guardrails, AWS Lambda, AWS Step Functions, Partner solutions, Technical How-to |\nThis post is co-written by Mark Warner, Principal Solutions Architect for Thales, Cyber Security Products.\nAs Generative AI applications move into production environments, they integrate with a wider scope of business systems that handle sensitive customer data. This integration introduces new challenges around protecting Personally Identifiable Information (PII) while maintaining data reversibility for downstream applications that legitimately require it. Consider a financial services company deploying generative AI across different departments. The customer service team needs an AI assistant that can access customer records and provide personalized responses including contact information. Meanwhile, the fraud analysis team requires the same customer data but must analyze patterns without exposing the actual PII, working only with protected representations of the sensitive information.\nAmazon Bedrock Guardrails helps detect sensitive information, such as PII, in a standard format within input prompts or model responses. Sensitive information filters give organizations control over how sensitive data is handled, with options to block requests containing PII or to mask sensitive information with generic placeholders like {NAME} or {EMAIL}.\nWhile masking effectively protects sensitive information, it introduces a new challenge: the loss of data reversibility. When guardrails replace sensitive data with generic masks, the original information becomes inaccessible to downstream applications that might need it for legitimate business processes.\nTokenization offers a complementary approach to this challenge. Unlike masking, tokenization replaces sensitive data with format-preserving tokens that are mathematically unrelated to the original information but still maintain its structure and usability. These tokens can be safely reversed back to their original value when needed by authorized systems, creating a path for secure data flow throughout an organization\u0026rsquo;s environment.\nIn this post, we show you how to integrate Amazon Bedrock Guardrails with third-party tokenization services to protect sensitive data while maintaining data reversibility. The solution described demonstrates how to combine Amazon Bedrock Guardrails with tokenization services from the Thales CipherTrust Data Security Platform to create an architecture that protects sensitive data without sacrificing the ability to process that data securely when needed.\nAmazon Bedrock Guardrails APIs Amazon Bedrock Guardrails offers two distinct approaches for implementing content safety controls:\nDirect integration with model invocation via APIs like InvokeModel and Converse. Independent evaluation via the ApplyGuardrail API, which decouples guardrail assessment from model invocation. This post utilizes the ApplyGuardrail API for tokenization integration because it separates the content assessment from the model call, allowing the insertion of the tokenization process between these steps.\nThe solution extends a typical ApplyGuardrail API deployment by inserting the tokenization processing between the guardrail assessment and the model invocation, as follows:\nThe application calls the ApplyGuardrail API to assess the user input for sensitive information. If sensitive information is detected (action = \u0026quot;ANONYMIZED\u0026quot;): The application captures the detected PII and its location. It calls a tokenization service to convert these entities into format-preserving tokens. It replaces the generic guardrail masks with these tokens. The application then calls the foundation model with the tokenized content. Solution Overview To illustrate how this workflow provides value in practice, consider a financial advisory application. Three separate application components work together to provide secure, AI-powered financial insights:\nCustomer gateway service: This trusted interface coordinator receives customer queries that often contain sensitive information. Financial analysis engine: This AI-powered component analyzes financial patterns and generates recommendations but does not need access to the actual customer PII. It operates with anonymized or tokenized information. Response processing service: This trusted service handles the final communication with the customer, including detokenizing sensitive information before presenting the result to the customer. The workflow is orchestrated by AWS Step Functions and uses AWS Lambda functions to sequence PII detection, tokenization, AI model invocation, and detokenization across the three main components:\nThe Customer gateway service receives user input $\\rightarrow$ Calls ApplyGuardrail API. Guardrail detects PII $\\rightarrow$ The gateway service calls the tokenization service (e.g., Thales CipherTrust) to generate format-preserving tokens. The input with tokenized values (e.g., [[TOKEN_123]]) is passed to the Financial analysis engine. The analysis engine calls an LLM on Amazon Bedrock to generate financial advice using the tokenized data. The model response (which may contain tokens) is sent to the Response processing service. This service calls the tokenization service to detokenize, restoring the original sensitive values. The final, detokenized response is sent to the customer. This architecture maintains data security throughout the processing flow while preserving the utility of the information.\nPrerequisites To deploy the described solution, you must have the following components configured in your environment:\nAn AWS account with Amazon Bedrock enabled. Appropriate AWS Identity and Access Management (IAM) permissions: bedrock:CreateGuardrail, bedrock:ApplyGuardrail, and bedrock-runtime:InvokeModel. A Python 3.7+ environment with the boto3 library and configured AWS credentials. A deployed tokenization service accessible via REST API endpoints (e.g., Thales CipherTrust Data Security Platform). Amazon Bedrock Guardrails Configuration Use the AWS SDK to configure the Guardrail with a sensitive information policy to ANONYMIZE PII types such as URL, EMAIL, and NAME for both inputs and outputs:\nimport boto3 def create_bedrock_guardrail(): \u0026#34;\u0026#34;\u0026#34; Create a guardrail in Amazon Bedrock for financial applications with PII protection. \u0026#34;\u0026#34;\u0026#34; bedrock = boto3.client(\u0026#39;bedrock\u0026#39;) response = bedrock.create_guardrail( name=\u0026#34;FinancialServiceGuardrail\u0026#34;, description=\u0026#34;Guardrail for financial applications with PII protection\u0026#34;, sensitiveInformationPolicyConfig={ \u0026#39;piiEntitiesConfig\u0026#39;: [ {\u0026#39;type\u0026#39;: \u0026#39;URL\u0026#39;, \u0026#39;action\u0026#39;: \u0026#39;ANONYMIZE\u0026#39;, \u0026#39;inputAction\u0026#39;: \u0026#39;ANONYMIZE\u0026#39;, \u0026#39;outputAction\u0026#39;: \u0026#39;ANONYMIZE\u0026#39;, \u0026#39;inputEnabled\u0026#39;: True, \u0026#39;outputEnabled\u0026#39;: True}, {\u0026#39;type\u0026#39;: \u0026#39;EMAIL\u0026#39;, \u0026#39;action\u0026#39;: \u0026#39;ANONYMIZE\u0026#39;, \u0026#39;inputAction\u0026#39;: \u0026#39;ANONYMIZE\u0026#39;, \u0026#39;outputAction\u0026#39;: \u0026#39;ANONYMIZE\u0026#39;, \u0026#39;inputEnabled\u0026#39;: True, \u0026#39;outputEnabled\u0026#39;: True}, {\u0026#39;type\u0026#39;: \u0026#39;NAME\u0026#39;, \u0026#39;action\u0026#39;: \u0026#39;ANONYMIZE\u0026#39;, \u0026#39;inputAction\u0026#39;: \u0026#39;ANONYMIZE\u0026#39;, \u0026#39;outputAction\u0026#39;: \u0026#39;ANONYMIZE\u0026#39;, \u0026#39;inputEnabled\u0026#39;: True, \u0026#39;outputEnabled\u0026#39;: True} ] }, blockedInputMessaging=\u0026#34;I can\u0026#39;t provide information with PII data.\u0026#34;, blockedOutputsMessaging=\u0026#34;I can\u0026#39;t generate content with PII data.\u0026#34; ) return response Tokenization Workflow Integration 1. Apply Guardrails to Detect PII Entities Use the ApplyGuardrail API to validate the input text:\ndef invoke_guardrail(user_query): #... (ApplyGuardrail API call logic) bedrock_runtime = boto3.client(\u0026#39;bedrock-runtime\u0026#39;) response = bedrock_runtime.apply_guardrail( guardrailIdentifier=\u0026#39;your-guardrail-id\u0026#39;, guardrailVersion=\u0026#39;your-guardrail-version\u0026#39;, source=\u0026#34;INPUT\u0026#34;, content=[{\u0026#34;text\u0026#34;: {\u0026#34;text\u0026#34;: user_query}}] ) return response 2. Invoke the Tokenization Service Parse the response from the Guardrail and call the third-party tokenization service (e.g., Thales CipherTrust) to convert the sensitive values into tokens.\nimport requests def thales_ciphertrust_tokenizer(guardrail_response): #... (logic to extract PII entities) #... (logic to call Thales CipherTrust /v1/protect endpoint) # Get sensitive_value and call API # response = requests.post(url_str, headers=headers, data=json.dumps(crdp_payload)) # protected_results.append({...}) #... pass 3. Replace Guardrail Masks with Tokens Replace the generic guardrail masks ({EMAIL}, {URL}, etc.) in the guardrail\u0026rsquo;s output with the generated format-preserving tokens.\ndef process_guardrail_output(protected_results, guardrail_response): #... (replacement logic) protection_map = {res[\u0026#39;type\u0026#39;].upper(): res[\u0026#39;protection_response\u0026#39;][\u0026#39;protected_data\u0026#39;] for res in protected_results} modified_outputs = [] for output_item in guardrail_response.get(\u0026#39;outputs\u0026#39;, []): if \u0026#39;text\u0026#39; in output_item: modified_text = output_item[\u0026#39;text\u0026#39;] for pii_type, protected_value in protection_map.items(): modified_text = modified_text.replace(f\u0026#34;{{{pii_type}}}\u0026#34;, protected_value) # Replace {PII_TYPE} with token modified_outputs.append({\u0026#34;text\u0026#34;: modified_text}) return modified_outputs Example of Sanitized Input:\nOriginal Input: \u0026quot;Hi, this is john.smith@example.com. Based on my last five transactions on acme.com...\u0026quot; Tokenized (Sanitized) Input: \u0026quot;Hi, this is 1001000GC5gDh1.D8eK71@EjaWV.lhC. Based on my last five transactions on 1001000WcFzawG.Jc9Tfc...\u0026quot; 4. Downstream Application Processing The financial analysis engine can call the LLM with the tokenized data to generate a relevant response. The response processing service then calls the tokenization service\u0026rsquo;s Detokenize API (e.g., Thales CipherTrust) to detokenize the tokens and restore the original values before sending the final response to the customer.\nCleanup To avoid incurring additional charges, delete the AWS resources you created after testing:\nDelete the guardrails you created (refer to the Amazon Bedrock documentation on deleting a guardrail). Delete the AWS Lambda, API Gateway, or Step Functions resources if you deployed the full workflow. Refer to the documentation for your third-party tokenization solution (e.g., Thales CipherTrust) for instructions on appropriate resource removal. Conclusion Combining Amazon Bedrock Guardrails with tokenization (e.g., Thales CipherTrust) provides a robust solution for protecting PII within generative AI workflows while preserving the necessary data utility and reversibility for authorized downstream applications. This approach helps organizations in tightly regulated industries balance generative AI innovation with compliance requirements.\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Artificial Intelligence: Running Deep Research AI Agents on Amazon Bedrock AgentCore Amazon Bedrock Blog Written by Vadim Omeltchenko, Eashan Kaushik, Mark Roy, and Shreyas Subramanian | September 23, 2025 | Category: Amazon Bedrock, Amazon Bedrock Agents, Generative AI, Technical How-to\nAI Agents are evolving beyond basic assistants performing single tasks to become more powerful systems capable of planning, critiquing, and collaborating with other agents to solve complex problems. Deep Agents—a new framework built on LangGraph—bring these capabilities to life, allowing multi-agent workflows to simulate real-world team dynamics.\nHowever, the challenge is not just building such agents but running them reliably and securely in a production environment. This is where Amazon Bedrock AgentCore Runtime comes into play. By providing a secure, purpose-built serverless environment for AI Agents and tools, the Runtime helps deploy Deep Agents at enterprise scale without the heavy lifting of infrastructure management.\nIn this post, we will demonstrate how to deploy Deep Agents on AgentCore Runtime. As shown in the diagram below, AgentCore Runtime scales any agent and provides session isolation by provisioning a new microVM for each new session.\nWhat is Amazon Bedrock AgentCore? Amazon Bedrock AgentCore is both framework-agnostic and model-agnostic, giving you the flexibility to deploy and operate advanced AI agents securely and at scale. Whether you are building with Strands Agents, CrewAI, LangGraph, LlamaIndex, or another framework—and running them on any large language model (LLM)—AgentCore provides the infrastructure to support them.\nIts modular services are purpose-built for dynamic agent workloads, with tools to extend agent capabilities and the controls necessary for production use. By removing the undifferentiated heavy lifting of building and managing specialized agent infrastructure, AgentCore allows you to bring your preferred framework and model and deploy without rewriting code.\nAmazon Bedrock AgentCore offers a comprehensive suite of capabilities designed to transform local agent prototypes into production-ready systems. These include:\nPersistent memory to maintain context within and across conversations. Access to existing APIs using the Model Context Protocol (MCP). Seamless integration with corporate authentication systems. Specialized tools for web browsing and code execution. Deep observability into agent reasoning processes. In this post, we focus specifically on the AgentCore Runtime component.\nCore Capabilities of AgentCore Runtime AgentCore Runtime provides a secure, serverless hosting environment designed specifically for agentic workloads. It packages code into a lightweight container with a simple, consistent interface, making it equally suitable for running agents, tools, MCP servers, or other workloads that benefit from seamless scaling and built-in identity management.\nAgentCore Runtime offers:\nExtended execution time of up to 8 hours for complex reasoning tasks. Handling of large payloads for multimodal content. Consumption-based pricing that charges only during active processing—not while waiting for LLM or tool responses. Dedicated micro virtual machines (microVMs) for each user session, ensuring complete isolation and preventing cross-session contamination. The Runtime works with multiple frameworks (e.g., LangGraph, CrewAI, Strands) and foundation model providers, while offering built-in corporate authentication, specialized agent observability, and unified access to the broader AgentCore environment via a single SDK.\nReal-World Example: Integrating Deep Agents In this post, we will deploy a newly released Deep Agents example implementation on AgentCore Runtime—showing how little effort is required to put the latest agent innovations into operation.\nThe example implementation includes:\nA research agent performing deep internet searches using the Tavily API. A critique agent reviewing and providing feedback on generated reports. A main orchestrator managing the workflow and handling file operations. Deep Agents utilize LangGraph\u0026rsquo;s state management to create a Multi-Agent system with:\nBuilt-in task planning via write_todos tool helping agents break down complex requests. Virtual file system where agents can read/write files to persist context between interactions. Sub-agent architecture allowing specialized agents to be invoked for specific tasks while maintaining context isolation. Recursive reasoning with a high recursion limit (over 1,000) to handle complex, multi-step workflows. This architecture allows Deep Agents to handle research tasks requiring multiple rounds of information gathering, synthesis, and refinement.\nCode Integration The beauty lies in its simplicity—we only need to add a few lines of code to make an agent compatible with AgentCore:\n# 1. Import the AgentCore runtime from bedrock_agentcore.runtime import BedrockAgentCoreApp app = BedrockAgentCoreApp() # 2. Decorate your agent function with @app.entrypoint @app.entrypoint async def langgraph_bedrock(payload): # Your existing agent logic remains unchanged user_input = payload.get(\u0026#34;prompt\u0026#34;) # Call your agent as before stream = agent.astream( {\u0026#34;messages\u0026#34;: [HumanMessage(content=user_input)]}, stream_mode=\u0026#34;values\u0026#34; ) # Stream responses back async for chunk in stream: yield(chunk) # 3. Add the runtime starter at the bottom if __name__ == \u0026#34;__main__\u0026#34;: app.run() That\u0026rsquo;s it! The rest of the code—model initialization, API integration, and agent logic—remains exactly the same. AgentCore handles the infrastructure while your agent handles the intelligence.\nStep-by-Step Implementation on AgentCore Runtime Let\u0026rsquo;s look at the actual deployment process using the AgentCore Starter ToolKit, which significantly simplifies the deployment workflow.\nPrerequisites Before starting, ensure you have:\nPython 3.10 or higher. AWS credentials configured. Amazon Bedrock AgentCore SDK installed. Step 1: IAM Permissions There are two different AWS Identity and Access Management (IAM) permissions to consider:\nDeveloper Role: Used by you to create AgentCore resources. Execution Role: Needed by the agent to run within the AgentCore Runtime. Note: The execution role can now be automatically created by the AgentCore Starter Toolkit (auto_create_execution_role=True).\nStep 2: Add a Wrapper to Your Agent As shown in the Deep Agents code snippet above, add the AgentCore imports and decorators to your existing agent code.\nStep 3: Deploy Using the AgentCore Starter Toolkit The starter toolkit provides a three-step deployment process:\nfrom bedrock_agentcore_starter_toolkit import Runtime # Step 1: Configure agentcore_runtime = Runtime() config_response = agentcore_runtime.configure( entrypoint=\u0026#34;hello.py\u0026#34;, # contains the code we showed earlier in the post execution_role=role_arn, # or auto-create auto_create_ecr=True, requirements_file=\u0026#34;requirements.txt\u0026#34;, region=\u0026#34;us-west-2\u0026#34;, agent_name=\u0026#34;deepagents-research\u0026#34; ) # Step 2: Launch launch_result = agentcore_runtime.launch() print(f\u0026#34;Agent deployed! ARN: {launch_result[\u0026#39;agent_arn\u0026#39;]}\u0026#34;) # Step 3: Invoke response = agentcore_runtime.invoke({ \u0026#34;prompt\u0026#34;: \u0026#34;Research the latest developments in quantum computing\u0026#34; }) Step 4: What Happens Behind the Scenes When you run the deployment, the toolkit automatically:\nCreates an optimized Docker file with a Python 3.13-slim base image and OpenTelemetry instrumentation. Builds your container with dependencies from requirements.txt. Creates an Amazon Elastic Container Registry (Amazon ECR) repository and pushes your image. Deploys to AgentCore Runtime and monitors deployment status. Configures networking and observability with Amazon CloudWatch and AWS X-Ray integration. The entire process typically takes 2–3 minutes. Each new session is launched in its own fresh AgentCore Runtime microVM.\nInvoking Your Deployed Agent Once deployed, you have two options to call your agent:\nOption 1: Using the Starter Toolkit\nresponse = agentcore_runtime.invoke({ \u0026#34;prompt\u0026#34;: \u0026#34;Research the latest developments in quantum computing\u0026#34; }) Option 2: Using the Boto3 SDK Directly\nimport boto3 import json agentcore_client = boto3.client(\u0026#39;bedrock-agentcore\u0026#39;, region_name=\u0026#39;us-west-2\u0026#39;) response = agentcore_client.invoke_agent_runtime( agentRuntimeArn=agent_arn, qualifier=\u0026#34;DEFAULT\u0026#34;, payload=json.dumps({ \u0026#34;prompt\u0026#34;: \u0026#34;Analyze the impact of AI on healthcare in 2024\u0026#34; }) ) # Handle streaming response for event in response[\u0026#39;completion\u0026#39;]: if \u0026#39;chunk\u0026#39; in event: print(event[\u0026#39;chunk\u0026#39;][\u0026#39;bytes\u0026#39;].decode(\u0026#39;utf-8\u0026#39;)) Deep Agents in Action When executing in Bedrock AgentCore Runtime, the main agent orchestrates specialized sub-agents. In this case, the orchestrator\u0026rsquo;s prompt sets the plan:\nWrite the question to question.txt. Scatter into one or more research agent calls (each for a unique sub-topic) using the internet_search tool. Synthesize findings into final_report.md. Call the critique-agent to evaluate gaps and structure. Optionally loop back for further research/editing. (Click on the drawing to play video)\nCleanup When finished, don\u0026rsquo;t forget to de-allocate the provisioned AgentCore Runtime and the container repository:\nagentcore_control_client = boto3.client( \u0026#39;bedrock-agentcore-control\u0026#39;, region_name=region ) ecr_client = boto3.client(\u0026#39;ecr\u0026#39;, region_name=region) runtime_delete_response = agentcore_control_client.delete_agent_runtime( agentRuntimeId=launch_result.agent_id ) response = ecr_client.delete_repository( repositoryName=launch_result.ecr_uri.split(\u0026#39;/\u0026#39;)[1], force=True ) Conclusion Amazon Bedrock AgentCore represents a paradigm shift in how we deploy AI agents. By abstracting infrastructure complexity while maintaining framework flexibility, it allows developers to focus on building sophisticated agent logic rather than managing deployment pipelines.\nOur Deep Agents implementation proves that even complex multi-agent systems can be deployed with minimal code changes. The combination of enterprise-grade security, built-in observability, and serverless scaling makes AgentCore the premier choice for production AI agent deployment.\nSpecifically for deep research agents, AgentCore offers unique capabilities:\nAsynchronous Processing: Handle long-running agents (up to 8 hours) without blocking responses. AgentCore Memory: Maintain complex investigation context without losing progress between sessions. AgentCore Gateway: Extend research to include proprietary insights from enterprise data sources via MCP tools. Ready to deploy?\nInstall the toolkit: pip install bedrock-agentcore-starter-toolkit Experiment: Deploy your code by following the step-by-step guide. The era of production-ready AI agents has arrived. With AgentCore, the journey from prototype to production has never been shorter.\nAbout the Authors Vadim Omeltchenko is a Sr. AI/ML Solutions Architect, passionate about helping AWS customers innovate in the cloud.\nEashan Kaushik is a Specialist Solutions Architect for AI/ML at Amazon Web Services. He is driven by creating advanced generative AI solutions while prioritizing a customer-centric approach.\nShreyas Subramanian is a Principal Data Scientist, helping customers solve business challenges using Machine Learning on the AWS platform.\nMark Roy is a Principal Machine Learning Architect for AWS, helping customers design and build generative AI solutions. He leads solution architecture efforts for Amazon Bedrock launches.\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Introducing AWS Card Clash Mobile: Learn AWS Architecture Through Strategic Gameplay AWS Training and Certification Blog\nWritten by Curtis Morton | September 23, 2025 | Category: Announcements, AWS Training and Certification\nToday, we are excited to announce the availability of AWS Card Clash on mobile devices. This 3D virtual card game is designed to help you explore AWS Cloud Architecture and solution design. It is now available on both the Apple App Store and the Google Play Store.\nAWS Card Clash transforms complex cloud concepts into a fun and accessible learning experience for players at all skill levels.\nMaking Cloud Learning More Engaging Learning cloud architecture involves understanding how AWS services work together to form complete solutions. AWS Card Clash offers an interactive approach by turning AWS service knowledge into strategic gameplay, allowing you to:\nvisualize cloud architectures interact with AWS components build real-world skills How AWS Card Clash Works AWS Card Clash offers engaging turn-based gameplay centered around AWS architecture. Players must:\nidentify the correct AWS services place cards into appropriate architectural slots score points based on accuracy The immersive 3D environment puts you in the role of a solutions architect, completing architecture diagrams by adding missing components while learning common AWS deployment patterns.\nSpecial cards allow players to:\ndefend their strategies disrupt opponents\u0026rsquo; deployments gain competitive advantages Single-Player and Multiplayer Experiences AWS Card Clash supports:\nSingle-player mode against AI opponents Multiplayer battles against other players With quick, accessible matches, it’s perfect for learning on the go or during busy schedules.\nLearning Paths for Every Stage of Your Journey The game offers specialized learning paths for different skill levels:\nCloud Practitioner Path – ideal for beginners Solutions Architect Path – focuses on resilient and cost-optimized architecture Serverless Developer Path – teaches event-driven and serverless patterns Generative AI Path – explores AI and machine learning integrations With 57 unique architectural designs and progressively challenging scenarios, there’s content for everyone.\nLearning AWS Concepts Through Play AWS Card Clash is designed to:\nreinforce practical knowledge help build workplace-ready skills support AWS Certification exam preparation By repeatedly interacting with service combinations and architecture patterns, players become familiar with how AWS services work together in real-world solution scenarios.\nGet Started Whether you\u0026rsquo;re:\nbeginning your tech career advancing your AWS expertise or simply enjoy strategic games AWS Card Clash offers an enjoyable way to build cloud skills one card at a time.\nAWS Card Clash is available now on the Apple App Store and coming soon to Google Play.\nDownload the iOS version today or sign up for early access on Android.\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.1-workshop-overview/","title":"1. Workshop Overview","tags":[],"description":"","content":"Hands-on workshop Overview In this workshop, we will practice deploying a static website on Amazon S3 storage service and optimizing content delivery via Amazon CloudFront Content Delivery Network (CDN).\nKey Contents: Preparation (5.2): Create an S3 Bucket and upload sample website source code to the bucket. Configure S3 Static Hosting (5.3): Enable static website hosting feature on S3. Access Management (5.4 - 5.5): Configure Block Public Access and Access Control Lists (ACL) to allow users to access the website from the internet. Verify Website (5.6): Verify the website operation via the S3 Endpoint. CloudFront Integration (5.7): Initialize a CloudFront Distribution pointing to the S3 origin. Enhance security by blocking direct public access to S3 (Block Public Access) and routing traffic through CloudFront. Cleanup (5.8): Instructions on deleting created resources to avoid incurring costs. "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.7-cloudfront/5.7.1-block-public-access/","title":"Block Public Access","tags":[],"description":"","content":" In S3 Bucket Interface Select Permissions Currently, the Block all public access function is Off because we disabled it in step 4 Select Edit Select Block all public access Select Save changes - Another window appears to confirm the edit, type confirm - Select Confirm\n- Block all public access is now enabled, at this point no public access can connect to your S3 Bucket.\nCheck Block all public access function In S3 bucket interface Select Properties - Scroll to the bottom of the page, at the Static website hosting section - Select the square icon to copy URL\n- Open URL in a new browser tab\n-\u0026gt; Congratulations, you have successfully Blocked public access\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.2-prerequiste/5.2.1-create-s3-bucket/","title":"Create S3 Bucket","tags":[],"description":"","content":"Create S3 Bucket What is an S3 Bucket? An S3 bucket is a container for objects stored in Amazon S3. Think of it as a top-level folder containing your files. Each bucket has a globally unique name and exists in a specific AWS Region. Buckets serve as the fundamental organizing unit in S3 and provide a namespace for object storage.\nStep-by-Step Instructions Access S3 Service\nGo to AWS Management Console Find S3 using the search bar or by navigating to the Storage service category Select S3 Initiate Bucket Creation\nIn the S3 interface, select Create bucket Configure Basic Bucket Settings\nIn the Create bucket interface: Bucket name: Enter workshop-demo-092025 (if the name is taken, add a number or use a custom name) AWS Region: Select the region closest to your users or as required Object Ownership: Select ACLs disabled (recommended) Verify Bucket Creation Success! You have created an S3 bucket to store website source code. The bucket will be displayed under General purpose buckets\nWhat has been achieved Created a globally unique S3 bucket Configured default S3 settings Set up the foundation for static website hosting Next Step Next, we will upload the website source code to the S3 bucket and store it.\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Event Report: AWS Cloud Mastery Series #2 : DevOps on AWS Workshop Purpose of the Event Instill the DevOps Mindset: To move beyond tools and establish a deep understanding of the DevOps culture, principles, and key metrics (like DORA) that drive high-performing technology teams. Provide an End-to-End Technical Blueprint: To offer a comprehensive, full-day deep dive into the AWS DevOps toolchain, covering the entire software delivery lifecycle from source code to production monitoring. Master Automation Through Live Demos: To provide practical, hands-on demonstrations of core DevOps workflows, including building a full CI/CD pipeline, deploying infrastructure as code (IaC), and managing containerized applications. Build Resilient and Observable Systems: To equip attendees with the knowledge to not only deploy applications efficiently but also to monitor, trace, and manage them effectively in a complex, distributed environment. List of Speakers Truong Quang Tinh - DevOps Engineer, TymeX Van Hoang Kha - Cloud Security Engineer, AWS Vietnam Quoc Bao - AWS Vietnam Phuc Thinh - AWS Vietnam Dai Vi - AWS Vietnam Long Huynh - AWS Vietnam Quy Pham - AWS Vietnam Nghiem Le - AWS Vietnam Key Content \u0026amp; Highlights From Code to Cloud: The Automated CI/CD Pipeline Focus: The morning\u0026rsquo;s core session was dedicated to building a fully automated software delivery pipeline from scratch. Content: The instructors provided a deep dive into the AWS CodeSuite, covering: Source: AWS CodeCommit and best practices for Git strategies (GitFlow vs. Trunk-based). Build \u0026amp; Test: Configuring automated builds and integrating testing stages with CodeBuild. Deploy: A detailed look at CodeDeploy, comparing modern deployment strategies like Blue/Green, Canary, and Rolling updates. Orchestrate: Tying everything together into a seamless workflow with CodePipeline. Key Moment: The live demonstration of a complete CI/CD pipeline in action was a standout. It connected all the theoretical pieces into a tangible, working system, showing how a single code commit could automatically trigger the entire build, test, and deployment process. Related Session: \u0026ldquo;AWS DevOps Services – CI/CD Pipeline\u0026rdquo;. Building the Foundation: Infrastructure as Code (IaC) Focus: This session shifted from application code to infrastructure, teaching attendees how to manage cloud resources programmatically. Content: The workshop explored the two primary IaC tools on AWS: the declarative power of AWS CloudFormation (templates, stacks, drift detection) and the imperative flexibility of the AWS CDK, which allows developers to use familiar programming languages. Key Moment: The side-by-side demo of deploying infrastructure with both CloudFormation and the CDK provided immense clarity. The following discussion on when to choose each tool gave attendees a practical framework for making informed architectural decisions. Related Session: \u0026ldquo;Infrastructure as Code (IaC)\u0026rdquo;. Running Modern Applications: The World of Containers Focus: The afternoon began with a deep dive into containerization, the backbone of modern microservices architecture. Content: Attendees were taken on a journey from Docker fundamentals to managing containers at scale on AWS. This included securing images in Amazon ECR, comparing the orchestration power of Amazon ECS and EKS, and exploring the simplicity of AWS App Runner for simple deployments. Key Moment: The case study and demo comparing different microservices deployment strategies was incredibly valuable. It illustrated the real-world trade-offs between simplicity (App Runner), managed control (ECS), and ultimate flexibility (EKS). Related Session: \u0026ldquo;Container Services on AWS\u0026rdquo;. Closing the Loop: Full-Stack Observability Focus: This critical session taught attendees how to see inside their applications and infrastructure once they are live. Content: The presentation went beyond basic monitoring, covering the three pillars of observability: logs and metrics with Amazon CloudWatch, and distributed tracing with AWS X-Ray. Best practices for setting up meaningful alarms, dashboards, and on-call processes were also shared. Key Moment: The full-stack observability demo was like turning on the lights in a dark room. It showed how to trace a single user request as it traveled through multiple microservices, pinpointing bottlenecks and errors with precision. Related Session: \u0026ldquo;Monitoring \u0026amp; Observability\u0026rdquo;. What You Learned On Automation \u0026amp; Pipelines Build a Complete CI/CD Workflow: You learned how to construct an automated pipeline on AWS that takes code from a Git repository and safely deploys it to production using modern strategies like Blue/Green. Manage Infrastructure as Code: You now understand how to define and manage your entire cloud infrastructure programmatically, enabling version control, peer reviews, and repeatable deployments for your environments. You also know the key differences between CloudFormation and the CDK. On Modern Application Deployment Master Containerization on AWS: You gained a clear framework for choosing the right container service (ECS, EKS, or App Runner) based on your team\u0026rsquo;s skills, application complexity, and operational needs. Implement Advanced Deployment Strategies: You moved beyond simple deployments and learned how to use techniques like feature flags and A/B testing to release new features to users with minimal risk and maximum feedback. On System Reliability and Operations Build Observable Systems: You learned to create systems that are not just monitored, but truly observable. You can now use CloudWatch and X-Ray together to gain deep insights into application performance and quickly diagnose issues in a distributed architecture. Adopt DevOps Best Practices: You gained insights into proven practices for incident management, writing effective postmortems, and fostering a culture of continuous improvement within your team. Experience at the Event The full-day \u0026ldquo;DevOps on AWS\u0026rdquo; workshop was an intensive and incredibly rewarding masterclass. From the moment it began, there was a clear sense that this was not just a series of lectures, but a cohesive journey through the entire lifecycle of a modern application.\nThe Morning: Building the Blueprint The morning sessions felt like being handed the architectural blueprints for building a world-class software factory. The CI/CD pipeline demo wasn\u0026rsquo;t just a technical walkthrough; it was a moment of clarity where the abstract concept of automation became a tangible, powerful reality. The deep dive into IaC felt equally empowering, as if we were being given the tools to sculpt the cloud with code, ensuring consistency and eliminating manual errors forever.\nThe Afternoon: Navigating Real-World Complexity After lunch, the workshop shifted into the complex realities of running modern applications. The session on containers was a masterclass in navigating choices, clarifying the often-confusing landscape of ECS, EKS, and App Runner. The observability session was perhaps the most eye-opening part of the day. The demo showing how to trace a single request across a dozen services was a powerful illustration of how to find a needle in a haystack, transforming the daunting task of debugging into a systematic process.\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Event Report: AWS Edge Services Workshop Purpose of the Event Master Global Content Delivery: To provide a deep understanding of how to leverage Amazon CloudFront to architect and optimize global content delivery, reducing latency and improving user experience. Fortify Application Security: To equip attendees with the knowledge and tools to secure web applications against common threats, using AWS WAF to block malicious traffic, mitigate OWASP Top 10 vulnerabilities, and defend against bot attacks. Provide Hands-On, Practical Experience: To move beyond theory and allow attendees to apply what they’ve learned in a lab environment, implementing optimization and security configurations in real-world scenarios. Build a Secure and Optimized Foundation: To offer a comprehensive blueprint for using AWS Edge Services as the foundational layer for building robust, scalable, and secure web applications for a global audience. List of Speakers Nguyễn Gia Hưng - Head of Solutions Architect Julian Ju - Senior Edge Services Specialist Solutions Architect Kevin Lim Key Content \u0026amp; Highlights From Edge to Origin: CloudFront as Your Foundation Focus: The first major session established the core principles of using a Content Delivery Network (CDN) to accelerate and scale web applications. Content: The speaker, Nguyễn Gia Hưng, provided a comprehensive overview of Amazon CloudFront. The session covered: Global Architecture: How to utilize AWS\u0026rsquo;s global network of edge locations to serve content closer to users. Core CDN Capabilities: Caching strategies, optimizing content delivery, and integrating with origin servers like S3 or EC2. Security at the Edge: An introduction to security controls available directly within CloudFront to build a first line of defense. Key Moment: The clear, architectural diagrams showing how a user request is intelligently routed to the nearest edge location, instead of traveling all the way to the origin, was a foundational \u0026ldquo;aha!\u0026rdquo; moment. It perfectly illustrated the performance gains possible with a well-configured CDN. Related Session: \u0026ldquo;From Edge to Origin: CloudFront as Your Foundation\u0026rdquo;. Attack Surface Defense: WAF \u0026amp; Application Protection Focus: This session shifted from performance to security, demonstrating how to build a robust defense against a wide array of web-based threats. Content: Julian Ju delivered a deep dive into the AWS Web Application Firewall (WAF). Key topics included: Threat Mitigation: Using AWS WAF to block malicious traffic and mitigate common vulnerabilities like the OWASP Top 10 (e.g., SQL injection, Cross-Site Scripting). Bot Defense: Strategies and rulesets for identifying and defending against automated bot attacks that can scrape content, skew analytics, or attempt credential stuffing. Managed Rules \u0026amp; Customization: A look at using both AWS Managed Rules for quick protection and how to build custom rules for application-specific threats. Key Moment: The live demonstration of how easily a WAF rule could be configured to block a simulated SQL injection attack was incredibly powerful. It showed how a complex threat could be neutralized in minutes, providing immediate, tangible security value. Related Session: \u0026ldquo;Attack Surface Defense: WAF \u0026amp; Application Protection\u0026rdquo;. From Theory to Practice: Hands-On Workshops Focus: The entire afternoon was dedicated to practical application, allowing attendees to implement the concepts from the morning sessions in a guided lab environment. Content: Led by the full team of specialists, the labs were split into two core areas: Optimization: Attendees configured a CloudFront distribution, implemented caching policies, and worked through scenarios to optimize content delivery for a sample web application. Security: The second lab focused on deploying AWS WAF, applying rule sets to protect against common attacks, and observing how the firewall blocked simulated malicious requests in real-time. Key Moment: The hands-on security lab was a standout. After learning about threats in the morning, actually configuring the WAF and then running an attack script to see it get blocked provided immense satisfaction and cemented the day\u0026rsquo;s learnings. Discussing real-world implementations with the specialists during the lab was invaluable. Related Sessions: \u0026ldquo;Hands On Workshop: Optimize Internet Web Application\u0026rdquo; \u0026amp; \u0026ldquo;Hands On Workshop: Secure Internet Web Application\u0026rdquo;. What You Learned On Performance \u0026amp; Global Scale Architect for Low Latency: You learned how to use Amazon CloudFront to design a content delivery strategy that serves users from edge locations around the world, drastically improving your application\u0026rsquo;s load times and responsiveness. Implement Caching Strategies: You now understand how to configure caching policies to reduce the load on your origin servers, lower data transfer costs, and build a more scalable and resilient application foundation. On Application Security Deploy a Web Application Firewall: You gained the practical skills to deploy and configure AWS WAF to protect your applications from common vulnerabilities and malicious traffic. Mitigate Common Threats: You are now equipped to mitigate threats listed in the OWASP Top 10 and defend your application against harmful automated bots, securing your data and your users. On Practical Implementation Bridge Theory and Practice: Through the hands-on labs, you translated theoretical knowledge about CDNs and WAFs into practical, repeatable skills that you can apply directly to your own projects. Solve Real-World Scenarios: You worked through realistic scenarios for both optimizing and securing a web application, gaining experience in making trade-offs and implementing effective solutions with expert guidance. Experience at the Event The full-day \u0026ldquo;AWS Edge Services Workshop\u0026rdquo; was an intensive and incredibly rewarding masterclass. From the moment it began, there was a clear sense that this was not just a series of lectures, but a cohesive journey through the entire lifecycle of securing and accelerating a modern web application.\nThe Morning: Building the Blueprint The morning sessions felt like being handed the architectural blueprints for building a digital fortress. The CloudFront session laid the groundwork, showing us how to build a super-fast, global \u0026ldquo;highway\u0026rdquo; to our application. Immediately following, the WAF session felt like learning how to construct the impenetrable \u0026ldquo;walls\u0026rdquo; and \u0026ldquo;gatehouses\u0026rdquo; to protect that highway from invaders. The logical progression from performance to security built a complete and powerful mental model.\nThe Afternoon: Navigating Real-World Complexity After lunch, the workshop shifted from blueprints to construction. The hands-on labs were where theory became tangible reality. Configuring a CloudFront distribution to speed up a sluggish site and then setting up WAF rules to fend off simulated attacks felt incredibly empowering. It was the difference between seeing a diagram of a shield and actually using one to block a blow. The ability to ask the specialists questions during the lab transformed it from a simple tutorial into a personalized coaching session, making the lessons stick.\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Event Report: AWS Cloud Mastery Series #3 : AWS Well-Architected Security Pillar Workshop Event Objectives Build a Solid Security Foundation: Provide a comprehensive overview of the AWS Well-Architected Security Pillar, helping businesses understand the importance of implementing security from the design phase (Security by Design). Master Core Principles: Dive deep into modern concepts such as Zero Trust, Least Privilege, and Defense in Depth to counter real-world threats in Vietnam. Master the 5 Security Pillars: Provide detailed technical guidance across 5 key aspects: Identity and Access Management (IAM), Detection, Infrastructure Protection, Data Protection, and Incident Response. Automate Security Processes: Shift the mindset from manual security to automation by utilizing AWS native services and the Detection-as-Code model. Speakers Huynh Hoang Long - AWS Vietnam Dinh Le Hoang Anh - AWS Vietnam Nguyen Tuan Thinh - AWS Vietnam Van Hoang Kha - Cloud Security Engineer, AWS Vietnam Thinh Lam - AWS Vietnam Viet Nguyen - AWS Vietnam Main Content \u0026amp; Highlights Identity \u0026amp; Access Management: From Traditional IAM to Modern Architecture Focus: This session focused on modernizing access management, shifting from using individual IAM Users to more centralized and secure solutions. Content: Core Principles: Emphasized the elimination of \u0026ldquo;long-term credentials\u0026rdquo; to minimize the risk of key exposure. IAM Identity Center: Introduced the Single Sign-On (SSO) model and permission sets to facilitate centralized access management across multiple accounts rather than fragmented management. Boundary Control: Utilizing Service Control Policies (SCP) to establish organization-level security guardrails. Key Highlight: The \u0026ldquo;Validate IAM Policy\u0026rdquo; Mini Demo was a standout moment. Witnessing the access simulation process helped identify over-privileged permission vulnerabilities that are hard to detect with the naked eye, affirming the importance of the \u0026ldquo;Least Privilege\u0026rdquo; principle in practice. Related Sessions: \u0026ldquo;Security Foundation\u0026rdquo;, \u0026ldquo;Modern IAM Architecture\u0026rdquo;. Monitoring and Infrastructure Protection: See Everything, Protect Every Layer Focus: Combining observability with network/host protection to create a multi-dimensional defense layer. Content: Detection: Implementing Logging at every layer (VPC Flow Logs, CloudTrail) and using GuardDuty for intelligent threat detection. The concept of \u0026ldquo;Detection-as-Code\u0026rdquo; was introduced to manage security rules as source code. Infrastructure Protection: Clearly distinguishing the roles of Security Groups and NACLs, along with applying WAF and Shield to mitigate DDoS attacks and web exploits. Key Highlight: The shift towards \u0026ldquo;Detection-as-Code\u0026rdquo; and automated alerting via EventBridge. This changes the mindset from passive \u0026ldquo;log watching\u0026rdquo; to a system that automatically reacts and notifies when anomalies occur in real-time. Related Sessions: \u0026ldquo;Detection \u0026amp; Continuous Monitoring\u0026rdquo;, \u0026ldquo;Network \u0026amp; Workload Security\u0026rdquo;. Data Protection \u0026amp; Incident Response: The Final Fortress and Action Plan Focus: Protecting the most valuable asset (Data) and standardized processes for handling incidents when they occur. Content: Encryption: Centralized key management with KMS, applying encryption both at-rest and in-transit. Managing Secrets using Secrets Manager to automatically rotate DB passwords. Incident Response (IR): Building specific Playbooks for scenarios such as: exposed IAM keys, public S3 buckets, or malware-infected EC2 instances. Key Highlight: The section on \u0026ldquo;Auto-response using Lambda/Step Functions.\u0026rdquo; The concept that the system can automatically isolate an infected EC2 instance or revoke an exposed IAM key without immediate human intervention is the clearest testament to the power of the cloud. Related Sessions: \u0026ldquo;Encryption, Keys \u0026amp; Secrets\u0026rdquo;, \u0026ldquo;IR Playbook \u0026amp; Automation\u0026rdquo;. Key Takeaways On Modern Security Mindset Zero Trust \u0026amp; Defense in Depth: A deep understanding that security is not just a firewall at the edge, but continuous authentication at every layer and having no default trust for any entity inside or outside the network. Shared Responsibility Model: Clearly grasping which parts AWS manages and which parts the business is responsible for, avoiding vulnerabilities caused by misunderstandings. On AWS Techniques \u0026amp; Tools Modern Identity Management: Knowing how to apply IAM Identity Center and SCP to manage at scale instead of struggling with individual IAM Users. Encryption Techniques \u0026amp; Secrets Management: Learned how to use KMS and Secrets Manager to eliminate hard-coding passwords in application source code – a common developer error. On Response Strategy IR Automation: Acquired the mindset of building \u0026ldquo;Playbooks\u0026rdquo; and turning them into automated processes, helping to reduce MTTR (Mean Time To Recovery) during security incidents. Learning Path: Grasped the roadmap for further security skill development (Security Specialty, SA Pro) to continue leveling up. Event Experience The \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; Workshop was a concise and highly practical training session. The event went beyond textbook theory, diving deep into actual \u0026ldquo;top threats\u0026rdquo; in Vietnam, helping attendees realize the urgency of the issues. The structure, moving from foundations to advanced automation solutions, provided a holistic picture: Security on AWS is not a barrier, but a key factor enabling businesses to innovate safely and sustainably.\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: AWS First Cloud Journey Community Day 2025 Event Objectives The AWS First Cloud Journey Community Day 2025 was organized to gather the cloud community, focusing on the practical application of cutting-edge solutions, especially Generative AI (GenAI), in critical sectors such as Finance/Banking, eCommerce, and Academia. The goal was to share reference architectures, practical models, and implementation experiences for Multi-Agent Systems and enterprise-grade chatbot solutions built on AWS Serverless infrastructure. To provide deep insights into data security tools and modern cloud architectures, including the use of the Model Context Protocol (MCP) for advanced context handling and automated auditing frameworks. To foster collaboration and knowledge sharing among developers, solution architects, and security experts. Featured Speakers Nguyễn Gia Hưng – Head of Solutions Architect, AWS Phạm Tiến Thuận Phát, Lê Minh Nghĩa, Trần Đoàn Công Lý (Enterprise Chatbot) Đinh Lê Hoàng Anh, Nguyễn Hồng Nhung, Nguyễn Tài Minh Huy (FinTech, Academic) Hải Trần (Banking / FinTech) Kiệt Lâm, Nguyễn Ngọc Quỳnh Mai (Banking / Internal IT) Nguyễn Việt Pháp (ISV / eCommerce) Lê Phạm Ngọc Uyển, Phan Thị Thanh Thảo, Hồ Điền Đăng Khoa, Nguyễn Quang Nhật Linh (GenAI Multi-Agent) Hiền Hà, Phát Phạm, Anh Phạm, Pháp Nguyễn, Việt Lý (Auto Audit, GenAI R\u0026amp;D) Trần Đức Anh, Lê Huỳnh Nghiêm, Nguyễn Anh Tuấn (Smart Data Contracts) Key Highlights 🚀 Advanced Chatbots and Context Handling (Floor 26) Enterprise Chatbot: Unlocking Context with MCP on AWS: This session demonstrated how to build enterprise chatbots capable of handling complex context and accessing backend systems via the Model Context Protocol (MCP), ensuring accurate and timely information. Internal Chatbot with RAG on AWS Serverless: A deep dive into deploying an internal FAQ chatbot using Retrieval-Augmented Generation (RAG), implemented entirely with AWS Serverless architecture for optimized cost and scalability. 🏦 AI Applications in Finance and Banking (Floor 36) Stock Trading Chatbot: Presented a serverless architecture for a stock trading chatbot, including AWS reference architecture patterns and a real-time demo of its market data analysis and response capabilities. Amazon Nova Act: The Revolutionary New AI Agent: Introduced cloud-native architecture solutions for banking workloads, focusing on scalability patterns and best practices when utilizing next-generation AI Agents. 🤖 Multi-Agent Systems and Automation (Floor 26 \u0026amp; 36) GenAI Multi-Agent Systems for Process Automation (Banking): Showcased case studies on using multi-agent systems to automate complex banking business processes, leveraging serverless workflows. Auto Audit Framework by AI: Introduced an AI-powered automated auditing framework, illustrating how multi-agent workflows can be orchestrated to execute real-world auditing use cases in the industry. 🌐 eCommerce, Academic, and Security (Floor 26 \u0026amp; 36) Scaling eCommerce with GenAI: Analyzed how GenAI can be used to enhance customer experience, product personalization, and optimize scaling performance on AWS for large eCommerce platforms. Smart Data Contracts Powered by GenAI: A crucial session on security, presenting how GenAI can be applied to build and validate secure smart data contracts, particularly in Fintech scenarios. Key Takeaways 1. GenAI and Enterprise Utility MCP is the Context Key: For enterprise chatbots to be truly functional, integrating the Model Context Protocol (MCP) is essential, allowing the model to access proprietary business data in a structured and secure manner, solving the \u0026ldquo;context loss\u0026rdquo; problem. Serverless RAG is the Standard: The Serverless RAG architecture (using AWS Lambda, S3, DynamoDB) is the most cost-effective and scalable deployment pattern for internal knowledge applications. 2. The Role of Multi-Agent Systems Automation of Complex Processes: Multi-agent is not just theoretical; it is a practical solution for breaking down and automating complex, multi-step processes such as internal auditing or financial approvals, where specialized agents collaborate. 3. Balancing Innovation and Compliance Deeply Integrated Security: Sessions emphasized the importance of architecting security from the ground up, from using agents for automated auditing to applying GenAI to build secure data contracts. "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Sources 2 Get to know FCJ members - Get to know team members, exchange and share information 8/9/2025 8/9/2025 3 Learn more about AWS: + Compute + Storage + Networking + \u0026hellip; 9/9/2025 9/9/2025 https://cloudjourney.awsstudygroup.com/ 4 Module 01 - 01 10/9/2025 10/9/2025 Module 01 - 01 5 Module 01 - 02 11/9/2025 11/9/2025 Module 01 - 02 6 Module 01 - 03 12/9/2025 12/9/2025 Module 01 - 03 Week 1 Achievements: Get along with the members of FCJ, mentors and the team.\nUnderstood the fundamentals of AWS and learned about its core service categories:\nCompute Storage Networking Database \u0026hellip; Completed some components of Module 01:\nExplored what cloud computing is and its core concept. Gained insight into the advantages of cloud computing, including reducing costs, speeding up development, flexible resource scaling, and expanding services worldwide. Built foundational knowledge of AWS and the upcoming cloud learning path. "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Finish Module 1 and Create AWS Account Tasks to be carried out this week: Day Task Start Date Completion Date Reference Sources 2 Module 01 - 04 15/9/2025 15/9/2025 Module 01 - 04 3 Module 01 - 05 16/9/2025 16/9/2025 Module 01 - 05 4 Module 01 - 06 17/9/2025 17/9/2025 Module 01 - 06 5 Module 01 - 07 18/9/2025 18/9/2025 Module 01 - 07 6 Complete Lab 01: Create AWS Account 19/9/2025 19/9/2025 Lab01 Week 2 Achievements: Finish Module 1\nGained knowledge of the AWS Global Infrastructure, including:\nData Centers Availability Zones (AZs) Regions (a region comprises a minimum of 3 AZs) Edge Locations CloudFront WAF (Web Application Firewall) Route 53 Explored AWS management tools:\nAWS Console - Root Login AWS Management Console - IAM Login Management Console - Service Search Management Console - Support Center AWS Command Line Interface (CLI) AWS SDKs (Software Development Kits) Learned about cost optimization methods when using AWS services:\nLeveraging discounted pricing models: Reserved Instances Savings Plans Spot Instances Practiced creating an AWS account. Conducted supplementary research on the AWS Well-Architected Framework. Successfully created and configured an AWS Free Tier account:\nSet up MFA (Multi-Factor Authentication) for the root account. Familiarized myself with creating an Admin Group and an Admin User. Configured authentication for the Admin User by:\nGenerating an Access Key. Managing the Secret Key (creation/deletion). "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Finish Module 2 Tasks to be carried out this week: Day Task Start Date Completion Date Reference Source Mon Module 02 - 01 22/9/2025 22/9/2025 Module 02 - 01 Tue Module 02 - 02 23/9/2025 23/9/2025 Module 02 - 02 Wed Module 02 - 03 24/9/2025 24/9/2025 Module 02 - 03 Thu Design appication with Figma 25/9/2025 25/9/2025 Fri Design appication with Figma 26/9/2025 26/9/2025 Week 3 Achievements: Explored, reviewed, and gained a solid grasp of VPC concepts\nDeveloped a clearer understanding of subnet types: public subnets and private subnets\nStudied various VPC-related components and services:\nRoute Tables Elastic Network Interfaces (ENIs) Elastic IP Addresses (EIPs) Internet Gateway NAT Gateway Examined security mechanisms within VPC:\nSecurity Groups Network Access Control Lists (NACLs) VPC Peering connections VPC Flow Logs Transit Gateway Learned about connectivity options between on-premises/data center infrastructures and AWS:\nSite-to-Site VPN AWS Direct Connect Studied the Elastic Load Balancing (ELB) service:\nSession stickiness (Sticky Sessions) Application Load Balancer (ALB) Network Load Balancer (NLB) Classic Load Balancer Gateway Load Balancer Gained knowledge of VPC Endpoints:\nUnderstood their core purpose and functionality Learned about the two main types of endpoints:\nInterface Endpoints Gateway Endpoints "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Complete Lab 03 Tasks to be carried out this week: Day Task Start Date Completion Date Reference Sources 2 Lab03-01 Lab03-01.1 Lab03-01.2 Lab03-01.3 Lab03-01.4 29/9/2025 29/9/2025 Lab03-01 Lab03-01.1 Lab03-01.2 Lab03-01.3 Lab03-01.4 3 Lab03-02.1 Lab03-02.2 Lab03-02.3 30/9/2025 30/9/2025 Lab03-02.1 Lab03-02.2 Lab03-02.3 4 Lab03-03.1 Lab03-03.2 Lab03-03.3 Lab03-03.4 Lab03-03.5 01/10/2025 01/10/2025 Lab03-03.1 Lab03-03.2 Lab03-03.3 Lab03-03.4 Lab03-03.5 5 Lab03-04.1 Lab03-04.2 Lab03-04.3 Lab03-04.5 02/10/2025 02/10/2025 Lab03-04.1 Lab03-04.2 Lab03-04.3 Lab03-04.5 6 Complete Lab 03 03/10/2025 03/10/2025 Lab03 Week 4 Achievements: Finished Lab 03 with the following accomplishments:\nGained hands-on experience setting up a VPC and configuring its subnets. Worked with Route Tables, updated Security Group settings, and explored how NACLs operate. Launched an EC2 instance and confirmed network connectivity. Used MobaXterm to establish the connection. Set up both a NAT Gateway and a VPC Endpoint. Also performed connectivity testing for the Endpoint. Note: To access the private instance, the inbound rules of its Security Group needed to be adjusted to allow SSH traffic coming from the Endpoint\u0026rsquo;s Security Group. "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Complete Lab 10, 19 Translate AWS Blogs Tasks to be carried out this week: Day Task Start Date Completion Date Reference Sources 2 - Complete Lab 10 06/10/2025 06/10/2025 Lab10 3 - Complete Lab 19 07/10/2025 07/10/2025 Lab19 4 - Translate Blog 1 08/10/2025 08/10/2025 Blog 1 5 - Translate Blog 2 09/10/2025 09/10/2025 Blog 2 6 - Translate Blog 3 10/10/2025 10/10/2025 Blog 3 Week 5 Achievements:= Completed Lab 10 with the following outcomes:\nReceived an overview of the AWS CloudFormation service. Gained hands-on practice by building a template, updating a Security Group (SG), and accessing an EC2 instance through a Remote Desktop Gateway Server (RDGW). Worked on setting up a Directory Service. Explored Route 53 functionality:\nConfigured a Route 53 Outbound Endpoint. Created DNS Resolver Rules. Set up an Inbound Endpoint for Route 53. Verified that all connections were working correctly afterward. Successfully established communication between an on-premises environment and the AWS cloud using Directory Service and a Remote Desktop Gateway Server. Completed Lab 19 with the following achievements:\nUnderstood the concept of VPC isolation and the purpose of establishing VPC Peering for secure, private communication between VPCs. Successfully configured a VPC Peering connection between two VPCs to enable direct traffic routing via private IP. Practiced working with Network ACLs as subnet-level stateless firewalls and updated ACL rules to allow required traffic. Implemented routing updates in Route Tables to enable bidirectional communication between the two VPCs. Configured Cross-Peer DNS to allow resources in each VPC to resolve DNS names across the peering connection. Completed the full workflow: preparation → updating Network ACLs → creating the Peering connection → configuring route tables → enabling Cross-Peer DNS. "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Design Project UI Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Design Project UI with Figma 13/10/2025 13/10/2025 3 - Design Project UI with Figma 14/10/2025 14/10/2025 4 - Design Project UI with Figma 15/10/2025 15/10/2025 5 - Design Project UI with Figma 16/10/2025 16/10/2025 6 - Design Project UI with Figma 17/10/2025 17/10/2025 Week 6 Achievements: "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Design Project UI Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Design Project UI with Figma 20/10/2025 20/10/2025 3 - Design Project UI with Figma 21/10/2025 21/10/2025 4 - Design Project UI with Figma 22/10/2025 22/10/2025 5 - Design Project UI with Figma 23/10/2025 23/10/2025 6 - Design Project UI with Figma 24/10/2025 24/10/2025 Week 7 Achievements: "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Start working on the final project. Prepare for the midterm exam. Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Started working on Final Project 27/10/2025 27/10/2025 3 - Work on Final Project 28/10/2025 28/10/2025 4 - Review for the midterm exam 29/10/2025 29/10/2025 5 - Review for the midterm exam 30/10/2025 30/10/2025 6 - Midterm exam 31/10/2025 31/10/2025 Week 8 Achievements: Initiated brainstorming sessions for the Final Project. Conducted research on the services the team plans to use in the Final Project. Designed the system architecture incorporating the AWS services selected for the Project. "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Continue working on the final project. Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Work on Final Project 03/11/2025 03/11/2025 3 - Work on Final Project 04/11/2025 04/11/2025 4 - Work on Final Project 05/11/2025 05/11/2025 5 - Work on Final Project 06/11/2025 06/11/2025 6 - Work on Final Project 07/11/2025 07/11/2025 Week 9 Achievements: "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Finish Module 1 and Create AWS Account\nWeek 3: Finish Module 2 and Design Project UI\nWeek 4: Complete Lab 03\nWeek 5: Finish Module 04 and Translate AWS Blogs\nWeek 6: Design Project UI\nWeek 7: Design Project UI\nWeek 8: Review and take mid-term exam\nWeek 9: Working on final Project\nWeek 10: Working on final Project\nWeek 11: Working on final Project\nWeek 12: Working on and finish final Project\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.2-prerequiste/","title":"2. Preparation Steps","tags":[],"description":"","content":"Objectives Create an S3 bucket Upload website resources to the S3 bucket Contents 5.2.1 - Create S3 bucket 5.2.2 - Upload resources "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.7-cloudfront/5.7.2-config-cloudfront/","title":"Configure CloudFront","tags":[],"description":"","content":"Use AWS Management Console to create a CloudFront distribution and configure this service to serve the S3 Bucket we created earlier.\nOpen CloudFront Console\nIn AWS Console, search for CloudFront From the dashboard, click on Create a CloudFront distribution. Configure CloudFront\nIn the plan selection interface, choose Free plan (note: this step might vary in newer console versions)\nSelect Next Distribution name: custom name (or workshop-demo)\nDistribution type: select Single website or app\nSelect Next On Specify Origin page\nOrigin type: select Amazon S3 Origin: select created S3 bucket Click Choose Then click Next Continue Next Review configuration, and then select Create distribution Then wait a few minutes for CloudFront to initialize\nGo to S3, into Permissions and scroll down to find Bucket policy Optional: If not working, try the following methods: Turn off S3 Static Website Hosting Then go to configure CloudFront Click on CloudFront ID In General, select Edit\nAdd index.html to Default root object\nThen select Save changes Then wait a few more minutes for CloudFront to re-initialize, and try opening the URL again in a new tab (or new browser) "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.2-prerequiste/5.2.2-download-source-code/","title":"Upload Resources","tags":[],"description":"","content":"Upload Data Download the source code we will use in this workshop.\nDownload link: Source code\nIn the interface of the newly created S3 bucket. Currently, we see there are no objects. Select Upload to upload data (the source code downloaded and extracted in the previous step) In the Upload interface Open the window containing the folders and files downloaded and extracted in the step above. Press Ctrl A to select all folders and files inside the workshop-demo-main directory. Drag all selected folders and files and drop them into the upload section of the S3 Bucket. Wait for about 5 minutes for the data upload to the S3 bucket to complete.\nInterface after upload is finished: View directory in S3 bucket "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/2-proposal/","title":"Proposal","tags":[],"description":"","content":"SnapResume Platform 1. Project Summary SnapResume is an intelligent resume/CV building platform powered by AI, helping job seekers create professional, ATS-friendly (Applicant Tracking System) resumes quickly and efficiently. The platform provides modern design templates, an intuitive editor, and an AI assistant to optimize content, fix grammar, and suggest keywords relevant to job descriptions.\nIts key differentiator is the integration of AI content generation (GenAI) using AWS Bedrock (Claude 3 Sonnet), allowing users to produce professional summaries and impressive experience descriptions from only basic bullet points. The system is fully built on a Serverless architecture, ensuring infinite scalability and optimized cost.\n2. Problem Statement What is the problem? Difficult formatting: Job seekers spend hours formatting text in Word or complex design tools but still fail to achieve a professional look. Weak content: Candidates struggle to write concise content, use industry terminology, and highlight achievements. “Writer’s block” is extremely common. Fails ATS screening: Manually designed resumes often contain graphics or table structures that employer ATS systems cannot parse. The Solution SnapResume solves these problems using a modern React web application built on AWS Serverless.\nVisual editor: Drag-and-drop UI or simple form-based input with real-time preview. AI Copilot: Uses AWS Bedrock to rewrite sentences and generate content tailored to the job role. ATS-Friendly: Templates designed to remain machine-readable while still visually appealing. Centralized management: Store multiple resume versions and export high-quality PDF. 3. Solution Architecture The system uses a Serverless and Event-driven architecture on AWS. Architecture Overview Frontend: React 19 + Vite (hosted on S3 + CloudFront or AWS Amplify). API Layer: Amazon API Gateway + AWS Lambda (Node.js/Express). Database: Amazon DynamoDB (Single Table Design) storing User, Resume, Section, Template. Authentication: Amazon Cognito (User Pools \u0026amp; Identity Pools). AI Engine: Amazon Bedrock (Claude 3 Sonnet) for content generation. Storage: Amazon S3 for storing profile images and exported PDF files. 4. Technical Implementation Technology Stack Frontend: ReactJS, TailwindCSS, Ant Design, html2pdf.js for PDF export. Backend: NodeJS, TypeScript, Express (running inside Lambda). IaC: Terraform managing all infrastructure. Development Phases Phase 1: Core Foundation (January) Set up AWS infrastructure (Terraform). Build Authentication (Cognito). Basic Resume CRUD (DynamoDB + Lambda). Phase 2: Editor \u0026amp; Templates (February) Develop drag-and-drop Editor UI. Build dynamic Template system. PDF export (html2pdf.js / Puppeteer). Phase 3: AI Integration \u0026amp; Polish (March) Integrate Amazon Bedrock for “AI Writer”. UX/UI optimization. Load testing \u0026amp; security hardening. 5. Budget Estimation (AWS Cost) The Serverless pricing model scales with usage. AI cost (Bedrock) becomes a major factor compared to traditional apps.\nAssumptions 1 Resume = 5 AI calls (optimization, rewriting descriptions). 1 AI call = 1,000 input tokens + 500 output tokens. Bedrock pricing (Claude 3 Sonnet): $3.00 / 1M input, $15.00 / 1M output. AI cost per Resume: ~ $0.05 (≈ 1,200 VND). PDF file size: 2MB. Service Pricing Model Low Traffic (MVP/Test) Medium Traffic High Traffic Scale \u0026lt; 500 users/month ~5,000/mo ~50,000/mo Resumes generated 200 CV 2,000 CV 20,000 CV Amazon S3 Storage $0.10 $1.00 $15.00 CloudFront CDN $0.50 $5.00 $50.00 API GW + Lambda Compute Free Tier $5.00 $40.00 DynamoDB Database Free Tier $2.00 $20.00 Cognito Auth Free Tier Free Tier Free Tier (\u0026lt; 50k MAU) Amazon Bedrock (AI) Tokens $10.00 $100.00 $1,000.00 WAF + Route53 Security $7.00 $7.00 $15.00 Total / Month ~ $18 ~ $120 ~ $1,140 Note: AI cost (Bedrock) becomes dominant at scale. Costs can be optimized by using smaller models such as Claude 3 Haiku for simple tasks, reducing AI cost to about 1/5.\n6. Risk Assessment Security \u0026amp; Privacy Risks (High) Issue: Resumes contain sensitive PII — phone number, email, address. Mitigation: Encrypt data at rest (DynamoDB, S3). Enforce strict access permissions (IAM Roles). User data is not used for AI training (AWS Bedrock does not train on customer inputs). Cost Risk (Medium) Issue: API DDoS attacks or AI abuse can incur high cost. Mitigation: Apply throttling on API Gateway. Limit daily AI calls per free-tier account. Set up AWS Budget Alerts. Architectural Risk Issue: Lambda cold start may slow first-time user experience. Mitigation: Use Lambda SnapStart (for Java) or Provisioned Concurrency if needed (Node.js usually has fast cold start). 7. Expected Outcomes A complete SaaS platform targeting the recruitment market. Highly scalable system supporting tens of thousands of users without server management. AI integration provides a significant competitive advantage over traditional CV builders. "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Continue working on the final project. Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Work on Final Project 10/11/2025 10/11/2025 3 - Work on Final Project 11/11/2025 11/11/2025 4 - Work on Final Project 12/11/2025 12/11/2025 5 - Work on Final Project 13/11/2025 13/11/2025 6 - Work on Final Project 14/11/2025 14/11/2025 Week 10 Achievements: "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Attend the AWS Cloud Mastery #2 event. Complete the Project Proposal and the final report. Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Attend the AWS Cloud Mastery #2 event: DevOps on AWS 17/11/2025 17/11/2025 3 - Work on Final Proposal 18/11/2025 18/11/2025 4 - Attend AWS event about Edge Netwok Services 19/11/2025 19/11/2025 5 - Work on Final Proposal 20/11/2025 20/11/2025 6 - Work on Final Proposal 21/11/2025 21/11/2025 Week 11 Achievements: "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Complete the Project Proposal and the final report. Tasks to be carried out this week: Day Task Start Date Completion Date Resources 2 - Work on Final Project 24/11/2025 24/11/2025 3 - Work on Final Project 25/11/2025 25/11/2025 4 - Work on Final Project 26/11/2025 26/11/2025 5 - Work on Final Project 27/11/2025 27/11/2025 6 - Work on Final Project 28/11/2025 28/11/2025 Week 12 Achievements: "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Integrate tokenization with Amazon Bedrock Guardrails for secure data handling The blog demonstrates how to integrate Amazon Bedrock Guardrails with third-party tokenization services to secure Personally Identifiable Information within generative AI workflows. While Guardrails can only mask PII with generic placeholders, this integrated approach replaces those masks with format-preserving and reversible tokens. This architecture, orchestrated using AWS Step Functions and AWS Lambda, allows downstream systems to operate on cryptographically protected yet structurally valid data, thereby maintaining both the security and the functional utility of the data throughout the entire processing pipeline.\nhttps://aws.amazon.com/blogs/machine-learning/integrate-tokenization-with-amazon-bedrock-guardrails-for-secure-data-handling/\nBlog 2 - Running deep research AI agents on Amazon Bedrock AgentCore The blog introduces the Amazon Bedrock AgentCore Runtime as a secure, serverless platform designed to deploy advanced AI Agents (such as LangGraph-based Deep Agents with multi-agent collaboration and complex reasoning capabilities) into production environments at scale. AgentCore tackles the infrastructure management challenge by providing a fully isolated environment for each session, while flexibly supporting any agent framework and LLM model. Using the AgentCore Starter Toolkit, developers can deploy complex systems with minimal code changes, allowing them them to focus on agent logic instead of worrying about enterprise-grade security, scalability, and features like persistent memory.\nhttps://aws.amazon.com/blogs/machine-learning/running-deep-research-ai-agents-on-amazon-bedrock-agentcore/\nBlog 3 - Introducing AWS Card Clash mobile: Learn AWS architecture through strategic gameplay The blog announces the mobile availability of AWS Card Clash, a 3D virtual card game designed to transform learning AWS Cloud architecture and solution design into an engaging and accessible experience. The game features strategic turn-based gameplay where players place AWS service cards into appropriate architectural slots to score points, thereby visualizing and recognizing common AWS deployment patterns. AWS Card Clash offers both single-player and multiplayer modes, with specialized learning paths including Cloud Practitioner, Solutions Architect, Serverless Developer, and Generative AI, helping players build practical knowledge and prepare for AWS certification exams.\nhttps://aws.amazon.com/blogs/training-and-certification/introducing-aws-card-clash-mobile-learn-aws-architecture-through-strategic-gameplay/\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.3-s3-static-web/","title":"3. Enable S3 Static Website Hosting","tags":[],"description":"","content":"In this section, you will enable static website hosting feature from the S3 bucket.\nStep-by-Step Configuration Access Bucket Properties In your S3 bucket interface, select the Properties tab Find Static Website Hosting In the Properties interface:\nScroll down to find the Static website hosting section Select Edit to modify settings Configure Website Hosting Settings\nStatic website hosting: Select Enable Hosting type: Select Host a static website Index document: Enter index.html Error document (optional): Enter error.html if you have a custom error page Save Configuration Note: After enabling static website hosting, your bucket will have a website endpoint URL, but the content will not be accessible until you:\nUpload your website files Configure public access Set up appropriate bucket policies Verify Configuration What has been achieved Enabled static website hosting from S3 bucket Set up website hosting configuration Obtained website endpoint URL "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.7-cloudfront/5.7.3-deploy-cloudfront/","title":"Deploy CloudFront","tags":[],"description":"","content":" Check CloudFront\nIn CloudFront interface, select ID. At Distribution domain name section:\nEnsure CloudFront has finished deploying by checking content in Last modified section. Select square icon to copy URL. Open URL in new browser tab -\u0026gt; Successfully deployed CloudFront\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.4-config-block-public/","title":"4. Configure Block Public Access","tags":[],"description":"","content":"Configure Block Public Access To access the hosted website, we need to reconfigure Block Public Access as follows:\nIn the S3 bucket interface: Select Permissions At this point, you will see Block all public access is in On state. In the Block public access interface: Uncheck Block all public access Select Save changes You have now completed turning off Block public access.\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/4-eventparticipated/","title":"Events Attended","tags":[],"description":"","content":"During my internship, I participated in four events. Each event was a memorable experience, offering new, interesting, and valuable knowledge, along with great gifts and wonderful moments.\nEvent 1 Event Name: Kick-off AWS FCJ Workforce OJT FALL 2025\nTime: 8:30 AM, Saturday, September 06, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, HCMC\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #1 : AI/ML/GenAI on AWS Workshop\nTime: 8:00 AM, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, HCMC\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #2 : DevOps on AWS Workshop\nTime: 8:00 AM, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, HCMC\nRole: Attendee\nEvent 4 Event Name: Workshop on AWS Edge Services\nTime: 09:00 AM, November 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, HCMC\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #3 : AWS Well-Architected Security Pillar Workshop\nTime: 08:30 AM, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, HCMC\nRole: Attendee\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.5-config-public-object/","title":"5. Configure Public Object","tags":[],"description":"","content":"Step-by-Step Configuration Access Bucket Permissions In your S3 bucket interface, select the Permissions tab Find Access Control List Settings Scroll down to find the Access control list (ACL) section\nYou will see Bucket owner enforced is currently selected This means ACLs are disabled and the bucket owner controls all objects Enable ACL for Object Level Control Select Edit in the Object Ownership section, then configure: Object ownership: Select ACLS enabled Acknowledgment: Check I acknowledge that ACLs will be restored Object ownership setting: Select Bucket owner preferred Select Save changes Verify ACL Configuration After saving, you will see ACLs enabled in the Object Ownership section.\nUpdated Configuration: Your bucket now supports ACLs, allowing you to set object-level permissions including public access. Make Objects Public Using ACL Navigate back to the Objects tab of the bucket:\nSelect the objects or folders you want to make public Select Actions from the toolbar Select Make public using ACL Confirm Public Access On the Make public confirmation page:\nReview the objects that will be made public Understand that these objects will be accessible by anyone Select Make public to confirm Final Warning: When you click “Make public”, these objects will immediately be accessible by anyone on the internet who knows the URL.\nVerify Public Configuration Success! Your objects are now publicly accessible.\n"},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/","title":"Workshop","tags":[],"description":"","content":"S3 Static Website \u0026amp; CloudFront Hosting System In this workshop, we will build a static website hosting and delivery system using Amazon S3 and Amazon CloudFront.\nSystem Architecture The system consists of the following key components:\nAmazon S3 Bucket: Stores website source code (HTML, CSS, JS, Images). Configured with Static Website Hosting feature. Amazon CloudFront: Content Delivery Network (CDN) to accelerate website access for global users and reduce load on S3. Security: Access management via Block Public Access and Bucket Policies/ACLs. Workshop Contents The practical labs are divided into sections for easy following:\nOverview (5.1): Introduction to the workshop and services used. Preparation (5.2): Create S3 Bucket and upload sample travel website source code. S3 Hosting Config (5.3): Enable Static Website Hosting on the bucket. Security Config (5.4 - 5.5): Configure Block Public Access and ACL to make the website public. Verify Website (5.6): Access the website via S3 Endpoint. CloudFront Integration (5.7): Deploy CloudFront CDN to distribute website content. Cleanup (5.8): Delete resources to avoid incurring costs. "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.6-check-website/","title":"6. Verify Website","tags":[],"description":"","content":"Verify Website After successfully installing and configuring, the next step is to check the website.\nAccess the created S3 bucket: Select Object. Select the file index.html Find details of index.html: Select Object URL. Open URL in a new browser tab: Experience the travel website interface. "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from 08/09/2025 to 12/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in DocuVerse Platform, through which I improved my skills in programming, analysis, reporting, communication, etc..\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ☐ ✅ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ☐ ✅ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Increase proactiveness in receiving and handling tasks. Improve problem-solving skills, analysis, and proposing better solutions. Enhance communication skills for clearer expression and more effective collaboration. Strengthen teamwork skills through discussion and supporting teammates. Expand professional knowledge. "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.7-cloudfront/","title":"7. CloudFront","tags":[],"description":"","content":"CloudFront is a CDN (Content Delivery Network) service from AWS. It helps reduce load for websites and applications by distributing content to locations closest to users.\nCloudFront has recently updated its new interface, as well as new features and services.\nIn this workshop, we will learn how to configure CloudFront and use it to distribute content to locations closest to users.\nContents 5.7.1 - Block Public Access 5.7.2 - Configure CloudFront 5.7.3 - Deploy CloudFront "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed.\nAdditional Questions What did you find most satisfying during your internship? Flexible schedule, the enthusiasm of the mentors. What do you think the company should improve for future interns?\nIncrease the amount of study seeions, events, ect.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nYes, because currently AWS has global coverage, having the opportunity to intern here would be fortunate, as the job opportunities are higher.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? No Would you like to continue this program in the future? Yes Any other comments (free sharing): "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/5-workshop/5.8-cleanup/","title":"8. Cleanup Resources","tags":[],"description":"","content":"Cleanup Resources Empty S3 Bucket Access AWS S3. In Bucket name list, select the bucket related to the lab. Select Empty. In Empty bucket page, confirm and select Empty. Delete S3 Bucket Access AWS S3. Select S3 bucket related to the lab. Select Delete bucket. Delete CloudFront Access Amazon CloudFront. Check the box before Distributions ID, on top right - select Disable. This process will take a few minutes. After Disable is complete, select Delete. "},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://dashork.github.io/AWS-FirstCloudJourney/tags/","title":"Tags","tags":[],"description":"","content":""}]